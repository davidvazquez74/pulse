name: Update News (hourly + impact + advanced geofilter)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout repo (full)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install helpers
        run: |
          python -m pip install --upgrade pip
          pip install charset-normalizer

      - name: Ensure folders + seed
        run: |
          mkdir -p data site/data
          if [ ! -f data/latest.json ]; then
            printf '%s\n' '{"updated_at":"1970-01-01T00:00:00Z","version":"seed","cataluna":[],"espana":[],"rioja":[],"global":[]}' > data/latest.json
          fi

      - name: Build latest.json (robust decode + impact + advanced geofilter)
        run: |
          python - <<'PY'
          import json, re, unicodedata, urllib.request, urllib.parse, xml.etree.ElementTree as ET
          from datetime import datetime, timezone
          from charset_normalizer import from_bytes as detect_charset

          FEEDS = {
            "cataluna": [
              "https://www.elperiodico.com/es/rss/catalunya/rss.xml",
              "https://www.lavanguardia.com/mvc/feed/rss/catalunya",
            ],
            "espana": [
              "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada",
              "https://www.rtve.es/rss/",
              "https://e00-elmundo.uecdn.es/elmundo/rss/espana.xml",
            ],
            "rioja": ["https://www.larioja.com/rss/2.0/portada"],
            "global": [
              "https://feeds.bbci.co.uk/news/world/rss.xml",
              "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
            ],
          }
          MAX = 3

          # ---------------- utils ----------------
          CTRL_RX = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f]')
          TAG_RX  = re.compile(r'<[^>]+>')
          IMG_RX  = re.compile(r'<img[^>]*>', re.I)

          def strip_html(s:str)->str:
              s = (s or '')
              s = IMG_RX.sub(' ', s)
              s = TAG_RX.sub(' ', s)
              s = s.replace('&nbsp;',' ').replace('&amp;','&')
              s = re.sub(r'\s+',' ', s).strip()
              return s

          def clean_txt(s):
              s = CTRL_RX.sub(' ', (s or ''))
              s = s.replace('\\n',' ')
              s = re.sub(r'\s+',' ', s).strip()
              return s

          def strip_accents(s:str)->str:
              if not s: return s
              nf = unicodedata.normalize('NFD', s)
              return ''.join(ch for ch in nf if unicodedata.category(ch) != 'Mn')

          def norm_lower_noacc(s:str)->str:
              return strip_accents((s or '').lower())

          def robust_text(url):
              req = urllib.request.Request(url, headers={"User-Agent":"PulseBot/1.0"})
              with urllib.request.urlopen(req, timeout=25) as resp:
                  data = resp.read()
                  enc = resp.headers.get_content_charset()
                  if not enc:
                      det = detect_charset(data).best()
                      enc = det.encoding if det else 'utf-8'
                  try:
                      text = data.decode(enc, errors='replace')
                  except Exception:
                      text = data.decode('latin-1', errors='replace')
                  return CTRL_RX.sub(' ', text).replace('\\\\','\\')

          def parse(xml_str, host):
              items=[]
              try:
                  root = ET.fromstring(xml_str.encode('utf-8'))
              except ET.ParseError:
                  return items
              # RSS
              for it in root.findall('.//item'):
                  title = strip_html(clean_txt(it.findtext('title'))) or '(sin título)'
                  link  = (it.findtext('link') or '').strip()
                  desc  = strip_html(it.findtext('description') or '')
                  pub   = datetime.now(tz=timezone.utc).isoformat()
                  items.append({"title": title, "url": link, "summary": desc,
                                "published_at": pub, "img": "", "source": host})
              # Atom
              for it in root.findall('.//{*}entry'):
                  title = strip_html(clean_txt(it.findtext('{*}title'))) or '(sin título)'
                  link_el = it.find('{*}link'); link = link_el.get('href','') if link_el is not None else ''
                  desc  = strip_html(it.findtext('{*}summary') or it.findtext('{*}content') or '')
                  pub   = datetime.now(tz=timezone.utc).isoformat()
                  items.append({"title": title, "url": link.strip(), "summary": desc,
                                "published_at": pub, "img": "", "source": host})
              return items

          # ---------------- impact engine ----------------
          KW = {
            r"(eur[ií]bor|bce|hipoteca|inflaci[oó]n)": ("finanzas",2,"este mes","vigilar"),
            r"(gasolina|di[eé]sel|petr[oó]leo|electricidad|gas)": ("energía",2,"esta semana","vigilar"),
            r"(huelga|paro).*(tren|metro|bus|vuelo)": ("movilidad",2,"hoy","planificar"),
            r"(alquiler|vivienda|inmobiliaria)": ("vivienda",2,"este mes","vigilar"),
            r"(impuestos|iva|subsidio|bono)": ("impuestos",2,"este mes","vigilar"),
            r"(meteocat|temperaturas?|ola de calor|vuelve el calor|maximas|minimas)": ("clima",2,"hoy","planificar"),
            r"(boletaire|setas?|micologia|bosque|medio natural)": ("medioambiente",2,"esta semana","vigilar"),
            r"(cadaver|muertos?|asesinato|violencia|derrumbre|colapso)": ("seguridad",1,"hoy","vigilar"),
            r"(concierto|festival|partido|maraton|celebracion)": ("eventos",2,"esta semana","planificar"),
            r"(champions|liga|mundial|copa|barca|real madrid|baloncesto|seleccionador)": ("deporte",2,"hoy","planificar"),
            r"(covid|vacuna|brote|epidemia|hospital)": ("salud",2,"esta semana","vigilar"),
            r"\bia\b|ciberseguridad|hackeo": ("tecnología",1,"esta semana","FYI"),
            r"empresa|fusion|resultados|cotizacion": ("negocios",1,"este mes","FYI"),
            r"juicio|sentencia|tribunal|fiscal": ("justicia",1,"esta semana","FYI"),
            r"incendio|contaminacion|vertido|desastre ambiental": ("medioambiente",2,"hoy","vigilar"),
            r"(palestina|onu|oriente proximo|macron|israel|gaza|cisjordania|hamas|reconocimiento del estado)": ("internacional",2,"esta semana","FYI"),
          }
          SENSITIVE = re.compile(r"(cadaver|muertos?|asesinato|violencia|epidemia|brote|covid|guerra|conflicto)", re.I)
          TAGS = {"finanzas","economia","negocios","energía","movilidad","vivienda","empleo","impuestos","clima",
                  "salud","educacion","eventos","deporte","cultura","seguridad","justicia","internacional","tecnología",
                  "medioambiente","transporte","salud_publica","otros"}

          def cw(s,maxw):
              w=re.split(r"\s+", (s or '').strip())
              return " ".join(w[:maxw])

          def impact(it):
              t=norm_lower_noacc(f"{it['title']} {it['summary']}")
              tag,sev,hz,act=("otros",0,"sin plazo","FYI")
              for rx,conf in KW.items():
                  if re.search(rx,t): tag,sev,hz,act=conf; break
              sensitive = SENSITIVE.search(t) is not None

              adult_map={
                "finanzas":"Revisa tu hipoteca y calcula cambios de cuota este mes.",
                "energía":"Compara precios y ajusta consumo esta semana para reducir factura.",
                "movilidad":"Planifica rutas y horarios alternativos hoy para evitar retrasos.",
                "vivienda":"Valora renegociar alquiler o retrasar decisiones este mes.",
                "impuestos":"Comprueba deducciones y plazos fiscales; evita recargos.",
                "clima":"Adapta ropa y horarios: calor/tiempo inestable esta semana.",
                "seguridad":"Evita la zona y sigue indicaciones oficiales.",
                "eventos":"Si asistes, prevé transporte y horarios con antelación.",
                "deporte":"Prevé afluencias; usa transporte público.",
                "salud":"Revisa pautas locales y evita aglomeraciones.",
                "tecnología":"Actualiza sistemas y activa verificación en dos pasos.",
                "negocios":"Monitorea costes/ingresos y ajusta planes.",
                "justicia":"Toma nota; posibles efectos de cumplimiento.",
                "medioambiente":"Respeta normas en bosques y evita áreas sensibles.",
                "internacional":"Sigue fuentes fiables; sin efecto inmediato en tu rutina.",
                "otros":"Sin efecto directo en tu día a día."
              }
              adult=cw(adult_map.get(tag,"Sin efecto directo en tu día a día."),22)

              if sensitive:
                  teen="Info importante. Sigue indicaciones; no difundas rumores."
              else:
                  teen_map={
                    "movilidad":"Organízate: puede haber lío en calles/metro. 🚇",
                    "eventos":"Llega con tiempo: colas y tráfico. 🎟️",
                    "deporte":"Habrá mucha gente; metro mejor. ⚽",
                    "energía":"Ojo a precios: controla gasto. 💡",
                    "finanzas":"Atento a tu cuota: puede cambiar. 💸",
                    "clima":"Pilla ropa ligera y agua. ☀️",
                    "medioambiente":"Cuida el bosque; deja todo limpio. 🍃",
                    "internacional":"Es noticia global; infórmate bien.",
                  }
                  teen=teen_map.get(tag,"Tranqui, solo para estar al día.")
              teen=cw(teen,18)
              if len(re.findall(r"[\U0001F300-\U0001FAFF\u2600-\u27BF]",teen))>1:
                  teen=re.sub(r"([\U0001F300-\U0001FAFF\u2600-\u27BF]).*$", r"\1", teen)

              conf=0.7 if tag!="otros" else 0.5
              if conf<0.5:
                  adult,teen,sev,act="Sin efecto directo en tu día a día.","Sin efecto directo.",0,"FYI"

              def dedupe(src,ttl):
                  ttl5=" ".join(ttl.split()[:6]).lower()
                  return src if ttl5 not in src.lower() else "Resumen breve relevante para ti."
              adult=dedupe(adult,it["title"]); teen=dedupe(teen,it["title"])

              if tag not in TAGS: tag="otros"
              return {"adult_impact":adult,"teen_impact":teen,"tag":tag,"severity":sev,"horizon":hz,"action":act,"rationale":f"Reglas por keywords; sensible={'sí' if sensitive else 'no'}; fuente={it['source']}", "confidence":conf}

          # ---------------- semantic filter config (tu JSON) ----------------
          PRIORITY = ["rioja","cataluna","espana","global"]

          SOURCE_BOOST = {
            "cataluna": ["lavanguardia.com","elperiodico.com","ara.cat","elnacional.cat","naciodigital.cat","324.cat","ccma.cat"],
            "rioja":    ["larioja.com","rioja2.com","nuevecuatrouno.com"],
            "espana":   ["elpais.com","rtve.es","elmundo.es","abc.es","larazon.es","elespanol.com"],
            "global":   ["bbc.co.uk","bbc.com","nytimes.com","cnn.com","dw.com","reuters.com","apnews.com"],
          }

          C_INC = set(map(norm_lower_noacc, [
            "cataluña","catalunya","catalan","generalitat","parlament","mossos","barcelona","bcn","girona","figueres",
            "lleida","lerida","tarragona","reus","tortosa","amposta","badalona","l'hospitalet","hospitalet","terrassa",
            "sabadell","granollers","manresa","igualada","mataro","sitges","vilafranca del penedes","penedes","barcelones",
            "baix llobregat","valles","vallès","maresme","anoia","garraf","emporda","alt emporda","baix emporda",
            "costa brava","costa daurada","pirineu","pirineos catalanes","rodalies","fgc","tmb","tram","port de barcelona",
            "el prat","aeropuerto el prat","diada","procés","proces","soberanismo","independentismo","catsalut",
            "servei catala de la salut","ics","bombers de la generalitat","tsjc","barça","fc barcelona","rcd espanyol",
            "montjuic","sants"
          ]))
          C_EXC = set(map(norm_lower_noacc, ["la rioja","riojano","riojana","logroño","calahorra","arnedo"]))

          E_INC = set(map(norm_lower_noacc, [
            "españa","gobierno","moncloa","congreso","senado","boe","ministerio","ministro","ministra","guardia civil",
            "policia nacional","cni","audiencia nacional","tribunal supremo","tribunal constitucional","ibex 35","ine",
            "aena","renfe","adif","dgt","aemet","seguridad social","sepe","hacienda","agencia tributaria","tesoro",
            "cnmc","cnmv","banco de españa","zarzuela","casa real","elecciones generales","ley organica","real decreto",
            "pp","psoe","vox","sumar","podemos","comunidades autonomas","ccaa","delegacion del gobierno","defensa",
            "interior","sanidad","educacion","transportes","transicion ecologica","economia"
          ]))
          E_EXC = set(map(norm_lower_noacc, list(C_EXC) + [
            "haro","najera","alfaro","ezcaray"
          ]))

          R_INC = set(map(norm_lower_noacc, [
            "la rioja","riojano","riojana","logroño","calahorra","arnedo","haro","najera","nájera","alfaro","ezcaray",
            "santo domingo de la calzada","cenicero","briones","san millan de la cogolla","san millán de la cogolla",
            "villamediana de iregua","lardero","murillo de rio leza","navarrete","cameros","iber","najerilla",
            "parlamento de la rioja","gobierno de la rioja","seris","hospital san pedro","universidad de la rioja","ur",
            "policia local de logroño","ayuntamiento de logroño","rio ebro (logroño)","feria de la vendimia","san mateo (logroño)"
          ]))
          R_EXC_WINE = set(map(norm_lower_noacc, [
            "doca rioja","denominacion de origen rioja","bodega","enoturismo","exportaciones de vino","vino","uvas","enologia","enológica"
          ]))

          G_INC = set(map(norm_lower_noacc, [
            "internacional","union europea","ue","comision europea","parlamento europeo","bce","bruselas","estrasburgo","luxemburgo",
            "otan","onu","oms","oiea","g7","g20","estados unidos","eeuu","casa blanca","congreso estadounidense","reino unido","londres",
            "downing street","francia","paris","alemania","berlin","italia","roma","portugal","lisboa","rusia","moscu","ucrania",
            "china","pekin","beijing","shanghai","japon","corea del sur","india","israel","gaza","cisjordania","iran","irak","siria","libano",
            "mexico","argentina","brasil","colombia","chile","peru","canada","australia","sudafrica","kenia","nigeria"
          ]))

          def token_in(text:str, bag:set)->bool:
              return any(t in text for t in bag)

          def source_boost(host:str, url:str, region:str)->int:
              h = host or ''
              u = url or ''
              pts = 0
              for pat in SOURCE_BOOST.get(region, []):
                  if pat in h or pat in u:
                      pts += 2
              # paths específicos de elperiodico catalunya
              if region=="cataluna" and ("elperiodico.com/es/catalunya" in u):
                  pts += 2
              return pts

          def is_rioja_wine_only(text:str)->bool:
              # Aparece "rioja" en contexto de vino y NO aparecen topónimos/instituciones riojanas
              has_wine = token_in(text, R_EXC_WINE)
              has_topos = token_in(text, R_INC - R_EXC_WINE)
              has_rioja_word = ("rioja" in text)
              return has_rioja_word and has_wine and not has_topos

          def score_region(title:str, summary:str, host:str, url:str, region:str)->int:
              t = norm_lower_noacc(f"{title} {summary}")
              s = 0
              if region=="cataluna":
                  if token_in(t, C_INC): s += 3
                  if token_in(t, C_EXC): s -= 2
              elif region=="rioja":
                  if token_in(t, R_INC): s += 3
                  if is_rioja_wine_only(t): s -= 4  # override vino: no clasificar como rioja
              elif region=="espana":
                  if token_in(t, E_INC): s += 2
                  if token_in(t, E_EXC): s -= 1
              elif region=="global":
                  if token_in(t, G_INC): s += 2
              s += source_boost(host, url, region)
              return s

          def best_region(item):
              title, summary = item["title"], item["summary"]
              host = item["source"]; url = item.get("url","")

              scores = {reg: score_region(title, summary, host, url, reg)
                        for reg in ["rioja","cataluna","espana","global"]}
              max_score = max(scores.values())
              # umbral estándar
              if max_score < 2:
                  return "global"
              # empate -> prioridad
              winners = [r for r,v in scores.items() if v==max_score]
              for pr in PRIORITY:
                  if pr in winners:
                      return pr
              return "global"

          # ---------------- collect ----------------
          union=[]; bycat={}
          for cat,urls in FEEDS.items():
              all_it=[]
              for u in urls:
                  try:
                      text=robust_text(u); host=urllib.parse.urlparse(u).hostname
                      all_it.extend(parse(text,host))
                  except Exception as e:
                      print("Feed error:", u, e)
              bycat[cat]=all_it; union.extend(all_it)

          buckets={"cataluna":[], "espana":[], "rioja":[], "global":[]}
          def norm_key(t):
              t=norm_lower_noacc(t or "")
              t=re.sub(r"[^\w\s]"," ",t); t=re.sub(r"\s+"," ",t).strip()
              return t[:120]
          seen=set()
          for it in union:
              key=(norm_key(it["title"]), it["url"])
              if key in seen: continue
              seen.add(key)
              reg = best_region(it)
              buckets[reg].append(it)

          def intersect(lst):
              bag={}
              for it in lst:
                  k=norm_key(it["title"])
                  if k not in bag: bag[k]={"it":it,"cnt":0}
                  bag[k]["cnt"]+=1
              return [v["it"] for v in sorted(bag.values(), key=lambda x:x["cnt"], reverse=True)]

          ranked={cat:intersect(lst) for cat,lst in buckets.items()}
          def backfill(cat):
              need=MAX-len(ranked[cat])
              if need<=0:return
              more=buckets[cat]
              for it in more:
                  if it in ranked[cat]: continue
                  ranked[cat].append(it)
                  if len(ranked[cat])>=MAX: break
          for cat in ["cataluna","rioja","espana","global"]:
              backfill(cat)

          # --------- impact + mapping ----------
          def map_item(cat,it):
              meta=impact(it)
              t=norm_lower_noacc(f"{it['title']} {it['summary']}")
              # Correcciones finales de tag
              if re.search(r"(narcotrafic|detenido|operacion policial|violencia)", t):
                  meta["tag"]="seguridad"
              if re.search(r"(palestina|onu|oriente proximo|macron|israel|gaza|cisjordania|hamas|reconocimiento del estado)", t):
                  meta["tag"]="internacional"
              if re.search(r"(meteocat|temperaturas?|ola de calor|maximas|minimas)", t):
                  meta["tag"]="clima"
              if re.search(r"(boletaire|setas?|micologia|bosque|medio natural)", t):
                  meta["tag"]="medioambiente"

              return {
                "title": it["title"], "summary": it["summary"], "url": it["url"],
                "source": it["source"], "published_at": it["published_at"], "img":"",
                "location": "global" if cat=="global" else ("ES" if cat=="espana" else ("Cataluña" if cat=="cataluna" else "La Rioja")),
                "category": meta["tag"], **meta
              }

          out={
            "updated_at": datetime.now(tz=timezone.utc).isoformat(),
            "version":"live",
            "cataluna":[map_item("cataluna",it) for it in ranked["cataluna"][:MAX]],
            "espana":[map_item("espana",it) for it in ranked["espana"][:MAX]],
            "rioja":[map_item("rioja",it) for it in ranked["rioja"][:MAX]],
            "global":[map_item("global",it) for it in ranked["global"][:MAX]],
          }

          for cat in out:
              if isinstance(out[cat], list):
                  for it in out[cat]:
                      for k in ['title','summary','adult_impact','teen_impact','source']:
                          if k in it: it[k] = clean_txt(it[k])

          with open('data/latest.json','w',encoding='utf-8') as f:
              json.dump(out,f,ensure_ascii=False,indent=2)
          print("Wrote data/latest.json")
          PY

      - name: Copy to site/data for static serving
        run: |
          cp -f data/latest.json site/data/latest.json
          echo "Copied data/latest.json -> site/data/latest.json"

      - name: Upload latest.json artifact
        uses: actions/upload-artifact@v4
        with:
          name: latest-json
          path: data/latest.json

      - name: Commit both JSONs
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/latest.json site/data/latest.json
          git commit -m "chore: update latest.json $(date -u +%FT%TZ)" || echo "No changes"
          git push || true
