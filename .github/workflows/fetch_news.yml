name: Update News (hourly + impact + advanced geofilter)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout repo (full)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install helpers
        run: |
          python -m pip install --upgrade pip
          pip install charset-normalizer

      - name: Ensure folders + seed
        run: |
          mkdir -p data site/data
          if [ ! -f data/latest.json ]; then
            printf '%s\n' '{"updated_at":"1970-01-01T00:00:00Z","version":"seed","cataluna":[],"espana":[],"rioja":[],"global":[]}' > data/latest.json
          fi

      - name: Build latest.json (robust decode + impact + advanced geofilter)
        run: |
          python - <<'PY'
          import json, re, unicodedata, urllib.request, urllib.parse, xml.etree.ElementTree as ET
          from datetime import datetime, timezone
          from charset_normalizer import from_bytes as detect_charset

          FEEDS = {
            "cataluna": [
              "https://www.elperiodico.com/es/rss/catalunya/rss.xml",
              "https://www.lavanguardia.com/mvc/feed/rss/catalunya",
            ],
            "espana": [
              "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada",
              "https://www.rtve.es/rss/",
              "https://e00-elmundo.uecdn.es/elmundo/rss/espana.xml",
            ],
            "rioja": ["https://www.larioja.com/rss/2.0/portada"],
            "global": [
              "https://feeds.bbci.co.uk/news/world/rss.xml",
              "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
            ],
          }
          MAX = 3

          # ---------------- utils ----------------
          CTRL_RX = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f]')
          TAG_RX  = re.compile(r'<[^>]+>')
          IMG_RX  = re.compile(r'<img[^>]*>', re.I)

          def strip_html(s:str)->str:
              s = (s or '')
              s = IMG_RX.sub(' ', s)
              s = TAG_RX.sub(' ', s)
              s = s.replace('&nbsp;',' ').replace('&amp;','&')
              s = re.sub(r'\s+',' ', s).strip()
              return s

          def clean_txt(s):
              s = CTRL_RX.sub(' ', (s or ''))
              s = s.replace('\\n',' ')
              s = re.sub(r'\s+',' ', s).strip()
              return s

          def strip_accents(s:str)->str:
              if not s: return s
              nf = unicodedata.normalize('NFD', s)
              return ''.join(ch for ch in nf if unicodedata.category(ch) != 'Mn')

          def norm_lower_noacc(s:str)->str:
              return strip_accents((s or '').lower())

          def robust_text(url):
              req = urllib.request.Request(url, headers={"User-Agent":"PulseBot/1.0"})
              with urllib.request.urlopen(req, timeout=25) as resp:
                  data = resp.read()
                  enc = resp.headers.get_content_charset()
                  if not enc:
                      det = detect_charset(data).best()
                      enc = det.encoding if det else 'utf-8'
                  try:
                      text = data.decode(enc, errors='replace')
                  except Exception:
                      text = data.decode('latin-1', errors='replace')
                  return CTRL_RX.sub(' ', text).replace('\\\\','\\')

          def parse(xml_str, host):
              items=[]
              try:
                  root = ET.fromstring(xml_str.encode('utf-8'))
              except ET.ParseError:
                  return items
              # RSS
              for it in root.findall('.//item'):
                  title = strip_html(clean_txt(it.findtext('title'))) or '(sin t√≠tulo)'
                  link  = (it.findtext('link') or '').strip()
                  desc  = strip_html(it.findtext('description') or '')
                  pub   = datetime.now(tz=timezone.utc).isoformat()
                  items.append({"title": title, "url": link, "summary": desc,
                                "published_at": pub, "img": "", "source": host})
              # Atom
              for it in root.findall('.//{*}entry'):
                  title = strip_html(clean_txt(it.findtext('{*}title'))) or '(sin t√≠tulo)'
                  link_el = it.find('{*}link'); link = link_el.get('href','') if link_el is not None else ''
                  desc  = strip_html(it.findtext('{*}summary') or it.findtext('{*}content') or '')
                  pub   = datetime.now(tz=timezone.utc).isoformat()
                  items.append({"title": title, "url": link.strip(), "summary": desc,
                                "published_at": pub, "img": "", "source": host})
              return items

          # ---------------- impact engine ----------------
          KW = {
            r"(eur[i√≠]bor|bce|hipoteca|inflaci[o√≥]n)": ("finanzas",2,"este mes","vigilar"),
            r"(gasolina|di[e√©]sel|petr[o√≥]leo|electricidad|gas)": ("energ√≠a",2,"esta semana","vigilar"),
            r"(huelga|paro).*(tren|metro|bus|vuelo)": ("movilidad",2,"hoy","planificar"),
            r"(alquiler|vivienda|inmobiliaria)": ("vivienda",2,"este mes","vigilar"),
            r"(impuestos|iva|subsidio|bono)": ("impuestos",2,"este mes","vigilar"),
            r"(meteocat|temperaturas?|ola de calor|vuelve el calor|maximas|minimas)": ("clima",2,"hoy","planificar"),
            r"(boletaire|setas?|micologia|bosque|medio natural)": ("medioambiente",2,"esta semana","vigilar"),
            r"(cadaver|muertos?|asesinato|violencia|derrumbre|colapso)": ("seguridad",1,"hoy","vigilar"),
            r"(concierto|festival|partido|maraton|celebracion)": ("eventos",2,"esta semana","planificar"),
            r"(champions|liga|mundial|copa|barca|real madrid|baloncesto|seleccionador)": ("deporte",2,"hoy","planificar"),
            r"(covid|vacuna|brote|epidemia|hospital)": ("salud",2,"esta semana","vigilar"),
            r"\bia\b|ciberseguridad|hackeo": ("tecnolog√≠a",1,"esta semana","FYI"),
            r"empresa|fusion|resultados|cotizacion": ("negocios",1,"este mes","FYI"),
            r"juicio|sentencia|tribunal|fiscal": ("justicia",1,"esta semana","FYI"),
            r"incendio|contaminacion|vertido|desastre ambiental": ("medioambiente",2,"hoy","vigilar"),
            r"(palestina|onu|oriente proximo|macron|israel|gaza|cisjordania|hamas|reconocimiento del estado)": ("internacional",2,"esta semana","FYI"),
          }
          SENSITIVE = re.compile(r"(cadaver|muertos?|asesinato|violencia|epidemia|brote|covid|guerra|conflicto)", re.I)
          TAGS = {"finanzas","economia","negocios","energ√≠a","movilidad","vivienda","empleo","impuestos","clima",
                  "salud","educacion","eventos","deporte","cultura","seguridad","justicia","internacional","tecnolog√≠a",
                  "medioambiente","transporte","salud_publica","otros"}

          def cw(s,maxw):
              w=re.split(r"\s+", (s or '').strip())
              return " ".join(w[:maxw])

          def impact(it):
              t=norm_lower_noacc(f"{it['title']} {it['summary']}")
              tag,sev,hz,act=("otros",0,"sin plazo","FYI")
              for rx,conf in KW.items():
                  if re.search(rx,t): tag,sev,hz,act=conf; break
              sensitive = SENSITIVE.search(t) is not None

              adult_map={
                "finanzas":"Revisa tu hipoteca y calcula cambios de cuota este mes.",
                "energ√≠a":"Compara precios y ajusta consumo esta semana para reducir factura.",
                "movilidad":"Planifica rutas y horarios alternativos hoy para evitar retrasos.",
                "vivienda":"Valora renegociar alquiler o retrasar decisiones este mes.",
                "impuestos":"Comprueba deducciones y plazos fiscales; evita recargos.",
                "clima":"Adapta ropa y horarios: calor/tiempo inestable esta semana.",
                "seguridad":"Evita la zona y sigue indicaciones oficiales.",
                "eventos":"Si asistes, prev√© transporte y horarios con antelaci√≥n.",
                "deporte":"Prev√© afluencias; usa transporte p√∫blico.",
                "salud":"Revisa pautas locales y evita aglomeraciones.",
                "tecnolog√≠a":"Actualiza sistemas y activa verificaci√≥n en dos pasos.",
                "negocios":"Monitorea costes/ingresos y ajusta planes.",
                "justicia":"Toma nota; posibles efectos de cumplimiento.",
                "medioambiente":"Respeta normas en bosques y evita √°reas sensibles.",
                "internacional":"Sigue fuentes fiables; sin efecto inmediato en tu rutina.",
                "otros":"Sin efecto directo en tu d√≠a a d√≠a."
              }
              adult=cw(adult_map.get(tag,"Sin efecto directo en tu d√≠a a d√≠a."),22)

              if sensitive:
                  teen="Info importante. Sigue indicaciones; no difundas rumores."
              else:
                  teen_map={
                    "movilidad":"Organ√≠zate: puede haber l√≠o en calles/metro. üöá",
                    "eventos":"Llega con tiempo: colas y tr√°fico. üéüÔ∏è",
                    "deporte":"Habr√° mucha gente; metro mejor. ‚öΩ",
                    "energ√≠a":"Ojo a precios: controla gasto. üí°",
                    "finanzas":"Atento a tu cuota: puede cambiar. üí∏",
                    "clima":"Pilla ropa ligera y agua. ‚òÄÔ∏è",
                    "medioambiente":"Cuida el bosque; deja todo limpio. üçÉ",
                    "internacional":"Es noticia global; inf√≥rmate bien.",
                  }
                  teen=teen_map.get(tag,"Tranqui, solo para estar al d√≠a.")
              teen=cw(teen,18)
              if len(re.findall(r"[\U0001F300-\U0001FAFF\u2600-\u27BF]",teen))>1:
                  teen=re.sub(r"([\U0001F300-\U0001FAFF\u2600-\u27BF]).*$", r"\1", teen)

              conf=0.7 if tag!="otros" else 0.5
              if conf<0.5:
                  adult,teen,sev,act="Sin efecto directo en tu d√≠a a d√≠a.","Sin efecto directo.",0,"FYI"

              def dedupe(src,ttl):
                  ttl5=" ".join(ttl.split()[:6]).lower()
                  return src if ttl5 not in src.lower() else "Resumen breve relevante para ti."
              adult=dedupe(adult,it["title"]); teen=dedupe(teen,it["title"])

              if tag not in TAGS: tag="otros"
              return {"adult_impact":adult,"teen_impact":teen,"tag":tag,"severity":sev,"horizon":hz,"action":act,"rationale":f"Reglas por keywords; sensible={'s√≠' if sensitive else 'no'}; fuente={it['source']}", "confidence":conf}

          # ---------------- semantic filter config (tu JSON) ----------------
          PRIORITY = ["rioja","cataluna","espana","global"]

          SOURCE_BOOST = {
            "cataluna": ["lavanguardia.com","elperiodico.com","ara.cat","elnacional.cat","naciodigital.cat","324.cat","ccma.cat"],
            "rioja":    ["larioja.com","rioja2.com","nuevecuatrouno.com"],
            "espana":   ["elpais.com","rtve.es","elmundo.es","abc.es","larazon.es","elespanol.com"],
            "global":   ["bbc.co.uk","bbc.com","nytimes.com","cnn.com","dw.com","reuters.com","apnews.com"],
          }

          C_INC = set(map(norm_lower_noacc, [
            "catalu√±a","catalunya","catalan","generalitat","parlament","mossos","barcelona","bcn","girona","figueres",
            "lleida","lerida","tarragona","reus","tortosa","amposta","badalona","l'hospitalet","hospitalet","terrassa",
            "sabadell","granollers","manresa","igualada","mataro","sitges","vilafranca del penedes","penedes","barcelones",
            "baix llobregat","valles","vall√®s","maresme","anoia","garraf","emporda","alt emporda","baix emporda",
            "costa brava","costa daurada","pirineu","pirineos catalanes","rodalies","fgc","tmb","tram","port de barcelona",
            "el prat","aeropuerto el prat","diada","proc√©s","proces","soberanismo","independentismo","catsalut",
            "servei catala de la salut","ics","bombers de la generalitat","tsjc","bar√ßa","fc barcelona","rcd espanyol",
            "montjuic","sants"
          ]))
          C_EXC = set(map(norm_lower_noacc, ["la rioja","riojano","riojana","logro√±o","calahorra","arnedo"]))

          E_INC = set(map(norm_lower_noacc, [
            "espa√±a","gobierno","moncloa","congreso","senado","boe","ministerio","ministro","ministra","guardia civil",
            "policia nacional","cni","audiencia nacional","tribunal supremo","tribunal constitucional","ibex 35","ine",
            "aena","renfe","adif","dgt","aemet","seguridad social","sepe","hacienda","agencia tributaria","tesoro",
            "cnmc","cnmv","banco de espa√±a","zarzuela","casa real","elecciones generales","ley organica","real decreto",
            "pp","psoe","vox","sumar","podemos","comunidades autonomas","ccaa","delegacion del gobierno","defensa",
            "interior","sanidad","educacion","transportes","transicion ecologica","economia"
          ]))
          E_EXC = set(map(norm_lower_noacc, list(C_EXC) + [
            "haro","najera","alfaro","ezcaray"
          ]))

          R_INC = set(map(norm_lower_noacc, [
            "la rioja","riojano","riojana","logro√±o","calahorra","arnedo","haro","najera","n√°jera","alfaro","ezcaray",
            "santo domingo de la calzada","cenicero","briones","san millan de la cogolla","san mill√°n de la cogolla",
            "villamediana de iregua","lardero","murillo de rio leza","navarrete","cameros","iber","najerilla",
            "parlamento de la rioja","gobierno de la rioja","seris","hospital san pedro","universidad de la rioja","ur",
            "policia local de logro√±o","ayuntamiento de logro√±o","rio ebro (logro√±o)","feria de la vendimia","san mateo (logro√±o)"
          ]))
          R_EXC_WINE = set(map(norm_lower_noacc, [
            "doca rioja","denominacion de origen rioja","bodega","enoturismo","exportaciones de vino","vino","uvas","enologia","enol√≥gica"
          ]))

          G_INC = set(map(norm_lower_noacc, [
            "internacional","union europea","ue","comision europea","parlamento europeo","bce","bruselas","estrasburgo","luxemburgo",
            "otan","onu","oms","oiea","g7","g20","estados unidos","eeuu","casa blanca","congreso estadounidense","reino unido","londres",
            "downing street","francia","paris","alemania","berlin","italia","roma","portugal","lisboa","rusia","moscu","ucrania",
            "china","pekin","beijing","shanghai","japon","corea del sur","india","israel","gaza","cisjordania","iran","irak","siria","libano",
            "mexico","argentina","brasil","colombia","chile","peru","canada","australia","sudafrica","kenia","nigeria"
          ]))

          def token_in(text:str, bag:set)->bool:
              return any(t in text for t in bag)

          def source_boost(host:str, url:str, region:str)->int:
              h = host or ''
              u = url or ''
              pts = 0
              for pat in SOURCE_BOOST.get(region, []):
                  if pat in h or pat in u:
                      pts += 2
              # paths espec√≠ficos de elperiodico catalunya
              if region=="cataluna" and ("elperiodico.com/es/catalunya" in u):
                  pts += 2
              return pts

          def is_rioja_wine_only(text:str)->bool:
              # Aparece "rioja" en contexto de vino y NO aparecen top√≥nimos/instituciones riojanas
              has_wine = token_in(text, R_EXC_WINE)
              has_topos = token_in(text, R_INC - R_EXC_WINE)
              has_rioja_word = ("rioja" in text)
              return has_rioja_word and has_wine and not has_topos

          def score_region(title:str, summary:str, host:str, url:str, region:str)->int:
              t = norm_lower_noacc(f"{title} {summary}")
              s = 0
              if region=="cataluna":
                  if token_in(t, C_INC): s += 3
                  if token_in(t, C_EXC): s -= 2
              elif region=="rioja":
                  if token_in(t, R_INC): s += 3
                  if is_rioja_wine_only(t): s -= 4  # override vino: no clasificar como rioja
              elif region=="espana":
                  if token_in(t, E_INC): s += 2
                  if token_in(t, E_EXC): s -= 1
              elif region=="global":
                  if token_in(t, G_INC): s += 2
              s += source_boost(host, url, region)
              return s

          def best_region(item):
              title, summary = item["title"], item["summary"]
              host = item["source"]; url = item.get("url","")

              scores = {reg: score_region(title, summary, host, url, reg)
                        for reg in ["rioja","cataluna","espana","global"]}
              max_score = max(scores.values())
              # umbral est√°ndar
              if max_score < 2:
                  return "global"
              # empate -> prioridad
              winners = [r for r,v in scores.items() if v==max_score]
              for pr in PRIORITY:
                  if pr in winners:
                      return pr
              return "global"

          # ---------------- collect ----------------
          union=[]; bycat={}
          for cat,urls in FEEDS.items():
              all_it=[]
              for u in urls:
                  try:
                      text=robust_text(u); host=urllib.parse.urlparse(u).hostname
                      all_it.extend(parse(text,host))
                  except Exception as e:
                      print("Feed error:", u, e)
              bycat[cat]=all_it; union.extend(all_it)

          buckets={"cataluna":[], "espana":[], "rioja":[], "global":[]}
          def norm_key(t):
              t=norm_lower_noacc(t or "")
              t=re.sub(r"[^\w\s]"," ",t); t=re.sub(r"\s+"," ",t).strip()
              return t[:120]
          seen=set()
          for it in union:
              key=(norm_key(it["title"]), it["url"])
              if key in seen: continue
              seen.add(key)
              reg = best_region(it)
              buckets[reg].append(it)

          def intersect(lst):
              bag={}
              for it in lst:
                  k=norm_key(it["title"])
                  if k not in bag: bag[k]={"it":it,"cnt":0}
                  bag[k]["cnt"]+=1
              return [v["it"] for v in sorted(bag.values(), key=lambda x:x["cnt"], reverse=True)]

          ranked={cat:intersect(lst) for cat,lst in buckets.items()}
          def backfill(cat):
              need=MAX-len(ranked[cat])
              if need<=0:return
              more=buckets[cat]
              for it in more:
                  if it in ranked[cat]: continue
                  ranked[cat].append(it)
                  if len(ranked[cat])>=MAX: break
          for cat in ["cataluna","rioja","espana","global"]:
              backfill(cat)

          # --------- impact + mapping ----------
          def map_item(cat,it):
              meta=impact(it)
              t=norm_lower_noacc(f"{it['title']} {it['summary']}")
              # Correcciones finales de tag
              if re.search(r"(narcotrafic|detenido|operacion policial|violencia)", t):
                  meta["tag"]="seguridad"
              if re.search(r"(palestina|onu|oriente proximo|macron|israel|gaza|cisjordania|hamas|reconocimiento del estado)", t):
                  meta["tag"]="internacional"
              if re.search(r"(meteocat|temperaturas?|ola de calor|maximas|minimas)", t):
                  meta["tag"]="clima"
              if re.search(r"(boletaire|setas?|micologia|bosque|medio natural)", t):
                  meta["tag"]="medioambiente"

              return {
                "title": it["title"], "summary": it["summary"], "url": it["url"],
                "source": it["source"], "published_at": it["published_at"], "img":"",
                "location": "global" if cat=="global" else ("ES" if cat=="espana" else ("Catalu√±a" if cat=="cataluna" else "La Rioja")),
                "category": meta["tag"], **meta
              }

          out={
            "updated_at": datetime.now(tz=timezone.utc).isoformat(),
            "version":"live",
            "cataluna":[map_item("cataluna",it) for it in ranked["cataluna"][:MAX]],
            "espana":[map_item("espana",it) for it in ranked["espana"][:MAX]],
            "rioja":[map_item("rioja",it) for it in ranked["rioja"][:MAX]],
            "global":[map_item("global",it) for it in ranked["global"][:MAX]],
          }

          for cat in out:
              if isinstance(out[cat], list):
                  for it in out[cat]:
                      for k in ['title','summary','adult_impact','teen_impact','source']:
                          if k in it: it[k] = clean_txt(it[k])

          with open('data/latest.json','w',encoding='utf-8') as f:
              json.dump(out,f,ensure_ascii=False,indent=2)
          print("Wrote data/latest.json")
          PY

      - name: Copy to site/data for static serving
        run: |
          cp -f data/latest.json site/data/latest.json
          echo "Copied data/latest.json -> site/data/latest.json"

      - name: Upload latest.json artifact
        uses: actions/upload-artifact@v4
        with:
          name: latest-json
          path: data/latest.json

      - name: Commit both JSONs
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/latest.json site/data/latest.json
          git commit -m "chore: update latest.json $(date -u +%FT%TZ)" || echo "No changes"
          git push || true
