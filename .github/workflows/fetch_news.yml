name: Update News (hourly)

on:
  workflow_dispatch:          # botón manual "Run workflow"
  schedule:
    - cron: "0 * * * *"       # cada hora

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Ensure folders + seed JSON
        run: |
          mkdir -p site/data
          if [ ! -f site/data/latest.json ]; then
            printf '%s\n' '{"updated_at":"1970-01-01T00:00:00Z","version":"seed","cataluna":[],"espana":[],"rioja":[],"global":[]}' > site/data/latest.json
          fi

      - name: Build latest.json (Python stdlib)
        shell: python
        run: |
          import json, re, urllib.request, xml.etree.ElementTree as ET
          from datetime import datetime, timezone

          FEEDS = {
            "cataluna": [
              "https://www.lavanguardia.com/mvc/feed/rss/catalunya",
              "https://www.elperiodico.com/es/rss/catalunya/rss.xml",
            ],
            "espana": [
              "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada",
              "https://www.rtve.es/rss/",
              "https://e00-elmundo.uecdn.es/elmundo/rss/espana.xml",
            ],
            "rioja": ["https://www.larioja.com/rss/2.0/portada"],
            "global": [
              "https://feeds.bbci.co.uk/news/world/rss.xml",
              "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
            ],
          }
          MAX = 3

          def fetch(url):
              req = urllib.request.Request(url, headers={"User-Agent":"PulseBot/1.0"})
              with urllib.request.urlopen(req, timeout=20) as r:
                  return r.read()

          def strip_html(s):
              return re.sub(r"<[^>]+>", "", s or "").strip()

          def to_iso(s):
              try:
                  return datetime.fromisoformat(s.replace("Z","+00:00")).astimezone(timezone.utc).isoformat()
              except Exception:
                  return datetime.now(tz=timezone.utc).isoformat()

          def parse_items(xml_bytes, source_host):
              items = []
              try:
                  root = ET.fromstring(xml_bytes)
              except ET.ParseError:
                  return items
              for it in root.findall(".//item"):
                  items.append({
                      "title": (it.findtext("title") or "").strip(),
                      "url": (it.findtext("link") or "").strip(),
                      "summary": strip_html(it.findtext("description") or ""),
                      "published_at": to_iso(it.findtext("pubDate") or ""),
                      "img": "",
                      "source": source_host,
                  })
              for it in root.findall(".//{*}entry"):
                  link_el = it.find("{*}link")
                  link = link_el.get("href","") if link_el is not None else ""
                  items.append({
                      "title": (it.findtext("{*}title") or "").strip(),
                      "url": link.strip(),
                      "summary": strip_html(it.findtext("{*}summary") or it.findtext("{*}content") or ""),
                      "published_at": to_iso(it.findtext("{*}updated") or ""),
                      "img": "",
                      "source": source_host,
                  })
              return items

          def norm_title(t):
              t = (t or "").lower()
              t = re.sub(r"[^\w\sáéíóúñüç]", " ", t)
              t = re.sub(r"\s+", " ", t).strip()
              return t[:80]

          def intersect_top(group):
              bag = {}
              for it in group:
                  k = norm_title(it["title"])
                  if k not in bag:
                      bag[k] = {"item": it, "cnt": 0, "latest": it["published_at"]}
                  bag[k]["cnt"] += 1
                  if it["published_at"] > bag[k]["latest"]:
                      bag[k]["latest"] = it["published_at"]
              arr = sorted(bag.values(), key=lambda x: (x["cnt"], x["latest"]), reverse=True)
              return [x["item"] for x in arr]

          def collect():
              out = {"updated_at": datetime.now(tz=timezone.utc).isoformat(), "version":"live",
                     "cataluna":[], "espana":[], "rioja":[], "global":[]}
              for cat, urls in FEEDS.items():
                  all_items = []
                  for u in urls:
                      try:
                          xml = fetch(u)
                          items = parse_items(xml, u.split("/")[2])
                          all_items.extend(items)
                      except Exception as e:
                          print("Feed error:", u, repr(e))
                  ranked = intersect_top(all_items)
                  used=set(); final=[]
                  for it in ranked:
                      k = norm_title(it["title"])
                      if k not in used:
                          used.add(k); final.append(it)
                      if len(final) >= MAX: break
                  if len(final) < MAX:
                      all_items.sort(key=lambda x:x["published_at"], reverse=True)
                      for it in all_items:
                          k = norm_title(it["title"])
                          if k not in used:
                              used.add(k); final.append(it)
                          if len(final) >= MAX: break
                  mapped=[]
                  for it in final[:MAX]:
                      mapped.append({
                          "title": it["title"], "summary": it["summary"], "url": it["url"],
                          "source": it["source"], "published_at": it["published_at"], "img": it.get("img",""),
                          "location": "global" if cat=="global" else ("ES" if cat=="espana" else ("Cataluña" if cat=="cataluna" else "La Rioja")),
                          "category": "otros"
                      })
                  out[cat]=mapped
              return out

          out = collect()
          with open("site/data/latest.json","w",encoding="utf-8") as f:
              json.dump(out,f,ensure_ascii=False,indent=2)
          print("Wrote site/data/latest.json")

      - name: Commit changes (if any)
        run: |
          if [[ -n "$(git status --porcelain site/data/latest.json)" ]]; then
            git config user.name "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            git add site/data/latest.json
            git commit -m "chore: update latest.json [skip ci]" || true
            git push || true
          else
            echo "No changes"
          fi
