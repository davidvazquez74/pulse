name: Update News (hourly + force-commit)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout repo (full)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Ensure folders + seed
        run: |
          mkdir -p data
          if [ ! -f data/latest.json ]; then
            printf '%s\n' '{"updated_at":"1970-01-01T00:00:00Z","version":"seed","cataluna":[],"espana":[],"rioja":[],"global":[]}' > data/latest.json
          fi
          ls -la data

      - name: Build latest.json (Python stdlib + backfill)
        run: |
          set -euo pipefail
          python - <<'PY'
          import json, re, urllib.request, urllib.parse, xml.etree.ElementTree as ET
          from datetime import datetime, timezone

          FEEDS = {
            "cataluna": [
              "https://www.elperiodico.com/es/rss/catalunya/rss.xml",
            ],
            "espana": [
              "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada",
              "https://www.rtve.es/rss/",
              "https://e00-elmundo.uecdn.es/elmundo/rss/espana.xml",
            ],
            "rioja": ["https://www.larioja.com/rss/2.0/portada"],
            "global": [
              "https://feeds.bbci.co.uk/news/world/rss.xml",
              "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
            ],
          }
          MAX = 3

          def fetch(url):
              req = urllib.request.Request(url, headers={"User-Agent":"PulseBot/1.0"})
              with urllib.request.urlopen(req, timeout=25) as r:
                  return r.read()

          def strip_html(s):
              import html
              s = html.unescape(s or "")
              return re.sub(r"<[^>]+>", "", s).strip()

          def safe_iso(s):
              try:
                  return datetime.fromisoformat((s or "").replace("Z","+00:00")).astimezone(timezone.utc).isoformat()
              except Exception:
                  return datetime.now(tz=timezone.utc).isoformat()

          def parse(xml_bytes, host):
              items = []
              try:
                  root = ET.fromstring(xml_bytes)
              except ET.ParseError:
                  return items
              for it in root.findall(".//item"):
                  items.append({
                      "title": (it.findtext("title") or "").strip() or "(sin título)",
                      "url": (it.findtext("link") or "").strip(),
                      "summary": strip_html(it.findtext("description") or ""),
                      "published_at": safe_iso(it.findtext("pubDate")),
                      "img": "",
                      "source": host,
                  })
              for it in root.findall(".//{*}entry"):
                  link_el = it.find("{*}link")
                  link = link_el.get("href","") if link_el is not None else ""
                  items.append({
                      "title": (it.findtext("{*}title") or "").strip() or "(sin título)",
                      "url": link.strip(),
                      "summary": strip_html(it.findtext("{*}summary") or it.findtext("{*}content") or ""),
                      "published_at": safe_iso(it.findtext("{*}updated") or it.findtext("{*}published")),
                      "img": "",
                      "source": host,
                  })
              return items

          def norm(t):
              t = (t or "").lower()
              t = re.sub(r"[^\w\sáéíóúñüç]", " ", t)
              t = re.sub(r"\s+", " ", t).strip()
              return t[:80]

          def intersect_top(lst):
              bag={}
              for it in lst:
                  k = norm(it["title"])
                  if k not in bag:
                      bag[k] = {"it": it, "cnt": 0, "latest": it["published_at"]}
                  bag[k]["cnt"] += 1
                  if it["published_at"] > bag[k]["latest"]:
                      bag[k]["latest"] = it["published_at"]
              return [v["it"] for v in sorted(bag.values(), key=lambda x: (x["cnt"], x["latest"]), reverse=True)]

          union_all=[]; by_cat={}
          for cat, urls in FEEDS.items():
              all_items=[]
              for u in urls:
                  try:
                      xml = fetch(u)
                      items = parse(xml, urllib.parse.urlparse(u).hostname)
                      all_items.extend(items); union_all.extend(items)
                      print(f"OK {cat} <- {u} : +{len(items)}")
                  except Exception as e:
                      print("Feed error:", u, repr(e))
              by_cat[cat]=all_items

          out = {"updated_at": datetime.now(tz=timezone.utc).isoformat(), "version":"live",
                 "cataluna":[], "espana":[], "rioja":[], "global":[]}

          def map_item(cat, it):
              return {
                "title": it["title"], "summary": it["summary"], "url": it["url"],
                "source": it["source"], "published_at": it["published_at"], "img": it.get("img",""),
                "location": "global" if cat=="global" else ("ES" if cat=="espana" else ("Cataluña" if cat=="cataluna" else "La Rioja")),
                "category": "otros"
              }

          for cat, all_items in by_cat.items():
              used=set(); final=[]
              top = intersect_top(all_items)
              for it in top:
                  k = norm(it["title"])
                  if k not in used:
                      used.add(k); final.append(it)
                  if len(final) >= MAX: break
              if len(final) < MAX:
                  all_items.sort(key=lambda x: x["published_at"], reverse=True)
                  for it in all_items:
                      k = norm(it["title"])
                      if k not in used:
                          used.add(k); final.append(it)
                      if len(final) >= MAX: break
              if not final and union_all:
                  union_all.sort(key=lambda x: x["published_at"], reverse=True)
                  for it in union_all:
                      k = norm(it["title"])
                      if k not in used:
                          used.add(k); final.append(it)
                      if len(final) >= MAX: break
              out[cat] = [map_item(cat, it) for it in final[:MAX]]

          print("Sizes:", {k: len(v) for k,v in out.items() if isinstance(v, list)})

          with open("data/latest.json","w",encoding="utf-8") as f:
              json.dump(out, f, ensure_ascii=False, indent=2)
          print("Wrote data/latest.json")
          PY

      - name: Show head (debug)
        run: |
          echo "----- HEAD data/latest.json -----"
          head -c 800 data/latest.json || true
          echo ""
          echo "--------------------------------"

      - name: Upload latest.json artifact
        uses: actions/upload-artifact@v4
        with:
          name: latest-json
          path: data/latest.json

      - name: Force commit (always push)
        run: |
          set -euo pipefail
          git config --global --add safe.directory "$GITHUB_WORKSPACE"
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          jq '.updated_at = (now | todateiso8601)' data/latest.json > data/latest.tmp && mv data/latest.tmp data/latest.json
          git add data/latest.json
          git status
          git commit -m "chore: update latest.json $(date -u +%FT%TZ)" || echo "No changes to commit"
          git push || true
          echo "Pushed data/latest.json"
