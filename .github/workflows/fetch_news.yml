name: Update News (hourly, python-stable)

on:
  workflow_dispatch:          # botón "Run workflow"
  schedule:
    - cron: "0 * * * *"       # cada hora

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Ensure folders + seed JSON
        run: |
          mkdir -p scripts data
          if [ ! -f data/latest.json ]; then
            printf '%s\n' '{"updated_at":"1970-01-01T00:00:00Z","version":"seed","cataluna":[],"espana":[],"rioja":[],"global":[]}' > data/latest.json
          fi

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Write build script (Python, no deps)
        run: |
          cat > scripts/build_latest.py <<'PY'
          import sys, json, re, time, urllib.request, urllib.error, xml.etree.ElementTree as ET
          from datetime import datetime, timezone

          FEEDS = {
              "cataluna": [
                  "https://www.lavanguardia.com/mvc/feed/rss/catalunya",
                  "https://www.elperiodico.com/es/rss/catalunya/rss.xml",
              ],
              "espana": [
                  "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada",
                  "https://www.rtve.es/rss/",
                  "https://e00-elmundo.uecdn.es/elmundo/rss/espana.xml",
              ],
              "rioja": [
                  "https://www.larioja.com/rss/2.0/portada",
              ],
              "global": [
                  "https://feeds.bbci.co.uk/news/world/rss.xml",
                  "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
              ],
          }
          MAX = 3

          def fetch(url, timeout=20):
              req = urllib.request.Request(url, headers={"User-Agent":"PulseBot/1.0"})
              with urllib.request.urlopen(req, timeout=timeout) as r:
                  return r.read()

          def text(el):
              return (el.text or "").strip() if el is not None else ""

          def strip_html(s):
              return re.sub(r"<[^>]+>", "", s or "").strip()

          def to_iso(s):
              try:
                  # Try HTTP-style dates first
                  return datetime.fromtimestamp(time.mktime(time.strptime(s[:31], "%a, %d %b %Y %H:%M:%S %Z")), tz=timezone.utc).isoformat()
              except Exception:
                  pass
              try:
                  # Fallback: any parsed date
                  return datetime.fromisoformat(s.replace("Z","+00:00")).astimezone(timezone.utc).isoformat()
              except Exception:
                  return datetime.now(tz=timezone.utc).isoformat()

          def parse_items(xml_bytes, source_host):
              # Works for RSS and Atom without external libs
              try:
                  root = ET.fromstring(xml_bytes)
              except ET.ParseError:
                  return []
              # namespaces (best-effort)
              ns = {}
              for k,v in root.attrib.items():
                  if k.startswith("xmlns"):
                      ns[k.split(":",1)[-1]] = v

              items = []
              # RSS <item>
              for it in root.findall(".//item"):
                  title = text(it.find("title"))
                  link = text(it.find("link"))
                  desc = text(it.find("description"))
                  pub  = text(it.find("pubDate"))
                  enclosure = it.find("enclosure")
                  img = enclosure.attrib.get("url","") if enclosure is not None else ""
                  items.append({
                      "title": title or "(sin título)",
                      "url": link,
                      "summary": strip_html(desc),
                      "published_at": to_iso(pub) if pub else datetime.now(tz=timezone.utc).isoformat(),
                      "img": img,
                      "source": source_host,
                  })
              # Atom <entry>
              if not items:
                  for it in root.findall(".//{*}entry"):
                      title = text(it.find("{*}title"))
                      link_el = it.find("{*}link")
                      link = link_el.attrib.get("href","") if link_el is not None else ""
                      summary = text(it.find("{*}summary")) or text(it.find("{*}content"))
                      updated = text(it.find("{*}updated")) or text(it.find("{*}published"))
                      items.append({
                          "title": title or "(sin título)",
                          "url": link,
                          "summary": strip_html(summary),
                          "published_at": to_iso(updated) if updated else datetime.now(tz=timezone.utc).isoformat(),
                          "img": "",
                          "source": source_host,
                      })
              return items

              # end parse_items

          def norm_title(t):
              t = (t or "").lower()
              t = re.sub(r"[^\w\sáéíóúñüç]", " ", t, flags=re.UNICODE)
              t = re.sub(r"\s+", " ", t).strip()
              return t[:80]

          def intersect_top(group):
              # Prioriza noticias repetidas entre feeds del mismo bloque
              bag = {}
              for it in group:
                  k = norm_title(it["title"])
                  if k not in bag:
                      bag[k] = {"item": it, "cnt": 0, "latest": it["published_at"]}
                  bag[k]["cnt"] += 1
                  if it["published_at"] > bag[k]["latest"]:
                      bag[k]["latest"] = it["published_at"]
              arr = sorted(bag.values(), key=lambda x: (x["cnt"], x["latest"]), reverse=True)
              return [x["item"] for x in arr]

          def collect():
              out = {"updated_at": datetime.now(tz=timezone.utc).isoformat(), "version":"live",
                     "cataluna":[], "espana":[], "rioja":[], "global":[]}
              for cat, urls in FEEDS.items():
                  all_items = []
                  for u in urls:
                      try:
                          xml = fetch(u)
                          items = parse_items(xml, urllib.request.urlparse(u).hostname)
                          all_items.extend(items)
                      except Exception as e:
                          print("Feed error:", u, repr(e), file=sys.stderr)
                          continue
                  ranked = intersect_top(all_items)
                  used = set()
                  final = []
                  for it in ranked:
                      k = norm_title(it["title"])
                      if k not in used:
                          used.add(k)
                          final.append(it)
                      if len(final) >= MAX:
                          break
                  if len(final) < MAX:
                      all_items.sort(key=lambda x: x["published_at"], reverse=True)
                      for it in all_items:
                          k = norm_title(it["title"])
                          if k not in used:
                              used.add(k)
                              final.append(it)
                          if len(final) >= MAX:
                              break
                  mapped = []
                  for it in final:
                      mapped.append({
                          "title": it["title"],
                          "summary": it["summary"],
                          "url": it["url"],
                          "source": it["source"],
                          "published_at": it["published_at"],
                          "img": it.get("img",""),
                          "location": "global" if cat=="global" else ("ES" if cat=="espana" else ("Cataluña" if cat=="cataluna" else "La Rioja")),
                          "category": "otros"
                      })
                  out[cat] = mapped
              return out

          def main():
              try:
                  out = collect()
              except Exception as e:
                  # Fallback que nunca rompe el job
                  out = {"updated_at": datetime.now(tz=timezone.utc).isoformat(), "version":"fallback",
                         "cataluna":[], "espana":[], "rioja":[], "global":[]}
                  print("Build FAILED, writing fallback JSON:", repr(e), file=sys.stderr)
              with open("data/latest.json","w",encoding="utf-8") as f:
                  json.dump(out, f, ensure_ascii=False, indent=2)
              print("Wrote data/latest.json")

          if __name__ == "__main__":
              main()
          PY

      - name: Build latest.json (Python)
        run: python scripts/build_latest.py

      - name: Commit changes (if any)
        run: |
          if [[ -n "$(git status --porcelain data/latest.json)" ]]; then
            git config user.name "github-actions[bot]"
            git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
            git add data/latest.json
            git commit -m "chore: update latest.json [skip ci]" || true
            git push || true
          else
            echo "No changes"
          fi
