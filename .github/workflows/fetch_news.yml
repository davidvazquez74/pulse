name: Update News (hourly + impact + geofilter + force-commit)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 * * * *"

permissions:
  contents: write

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout repo (full)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install helpers
        run: |
          python -m pip install --upgrade pip
          pip install charset-normalizer

      - name: Ensure folders + seed
        run: |
          mkdir -p data site/data
          if [ ! -f data/latest.json ]; then
            printf '%s\n' '{"updated_at":"1970-01-01T00:00:00Z","version":"seed","cataluna":[],"espana":[],"rioja":[],"global":[]}' > data/latest.json
          fi

      - name: Build latest.json (robust decode + impact + geofilter)
        run: |
          python - <<'PY'
          import json, re, urllib.request, urllib.parse, xml.etree.ElementTree as ET
          from datetime import datetime, timezone
          from charset_normalizer import from_bytes as detect_charset

          FEEDS = {
            "cataluna": [
              "https://www.elperiodico.com/es/rss/catalunya/rss.xml",
              "https://www.lavanguardia.com/mvc/feed/rss/catalunya",
            ],
            "espana": [
              "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada",
              "https://www.rtve.es/rss/",
              "https://e00-elmundo.uecdn.es/elmundo/rss/espana.xml",
            ],
            "rioja": ["https://www.larioja.com/rss/2.0/portada"],
            "global": [
              "https://feeds.bbci.co.uk/news/world/rss.xml",
              "https://rss.nytimes.com/services/xml/rss/nyt/World.xml",
            ],
          }
          MAX = 3

          CTRL_RX = re.compile(r'[\x00-\x08\x0b\x0c\x0e-\x1f]')
          def clean_txt(s):
              return CTRL_RX.sub(' ', (s or '')).replace('\\n',' ').replace('  ',' ').strip()

          def robust_text(url):
              req = urllib.request.Request(url, headers={"User-Agent":"PulseBot/1.0"})
              with urllib.request.urlopen(req, timeout=25) as resp:
                  data = resp.read()
                  enc = resp.headers.get_content_charset()
                  if not enc:
                      det = detect_charset(data).best()
                      enc = det.encoding if det else 'utf-8'
                  try:
                      text = data.decode(enc, errors='replace')
                  except Exception:
                      text = data.decode('latin-1', errors='replace')
                  return CTRL_RX.sub(' ', text).replace('\\\\','\\')

          def parse(xml_str, host):
              items=[]
              try:
                  root = ET.fromstring(xml_str.encode('utf-8'))
              except ET.ParseError:
                  return items
              for it in root.findall('.//item'):
                  items.append({
                      "title": clean_txt(it.findtext('title')) or '(sin título)',
                      "url": (it.findtext('link') or '').strip(),
                      "summary": clean_txt(it.findtext('description')),
                      "published_at": datetime.now(tz=timezone.utc).isoformat(),
                      "img": "", "source": host
                  })
              for it in root.findall('.//{*}entry'):
                  link_el = it.find('{*}link'); link = link_el.get('href','') if link_el is not None else ''
                  items.append({
                      "title": clean_txt(it.findtext('{*}title')) or '(sin título)',
                      "url": link.strip(),
                      "summary": clean_txt(it.findtext('{*}summary') or it.findtext('{*}content')),
                      "published_at": datetime.now(tz=timezone.utc).isoformat(),
                      "img": "", "source": host
                  })
              return items

          # Impact engine
          KW = {
            r"(eur[ií]bor|bce|hipoteca|inflaci[oó]n)": ("finanzas",2,"este mes","vigilar"),
            r"(gasolina|di[eé]sel|petr[oó]leo|electricidad|gas)": ("energía",2,"esta semana","vigilar"),
            r"(huelga|paro).*(tren|metro|bus|vuelo)": ("movilidad",2,"hoy","planificar"),
            r"(alquiler|vivienda|inmobiliaria)": ("vivienda",2,"este mes","vigilar"),
            r"(impuestos|iva|subsidio|bono)": ("impuestos",2,"este mes","vigilar"),
            r"(dana|temporal|lluvias|ola de calor|inundaci[oó]n)": ("clima",2,"hoy","planificar"),
            r"(cad[aá]ver|muertos?|asesinato|violencia)": ("seguridad",1,"hoy","vigilar"),
            r"(concierto|festival|partido|marat[oó]n|celebraci[oó]n)": ("eventos",2,"esta semana","planificar"),
            r"(champions|liga|mundial|copa|bar[cç]a|real madrid)": ("deporte",2,"hoy","planificar"),
            r"(covid|vacuna|brote|epidemia|hospital)": ("salud",2,"esta semana","vigilar"),
            r"\bIA\b|ciberseguridad|hackeo": ("tecnología",1,"esta semana","FYI"),
            r"empresa|fusi[oó]n|resultados|cotizaci[oó]n": ("negocios",1,"este mes","FYI"),
            r"juicio|sentencia|tribunal|fiscal": ("justicia",1,"esta semana","FYI"),
            r"incendio|contaminaci[oó]n|vertido|desastre ambiental": ("medioambiente",2,"hoy","vigilar"),
          }
          SENSITIVE = re.compile(r"(cad[aá]ver|muertos?|asesinato|violencia|epidemia|brote|covid)", re.I)
          TAGS = {"finanzas","economia","negocios","energía","movilidad","vivienda","empleo","impuestos","clima",
                  "salud","educacion","eventos","deporte","cultura","seguridad","justicia","internacional","tecnología",
                  "medioambiente","transporte","salud_publica","otros"}

          def cw(s,maxw):
              w=re.split(r"\\s+", (s or '').strip())
              return " ".join(w[:maxw])

          def impact(it):
              t=f"{it['title']} {it['summary']}".lower()
              tag,sev,hz,act=("otros",0,"sin plazo","FYI")
              for rx,conf in KW.items():
                  if re.search(rx,t): tag,sev,hz,act=conf; break
              sensitive = SENSITIVE.search(t) is not None

              adult_map={
                "finanzas":"Revisa tu hipoteca y calcula cambios de cuota este mes.",
                "energía":"Compara precios y ajusta consumo esta semana para reducir factura.",
                "movilidad":"Planifica rutas y horarios alternativos hoy para evitar retrasos.",
                "vivienda":"Valora renegociar alquiler o retrasar decisiones este mes.",
                "impuestos":"Comprueba deducciones y plazos fiscales; evita recargos.",
                "clima":"Evita desplazamientos innecesarios y protege exteriores hoy.",
                "seguridad":"Mantén prudencia y sigue indicaciones oficiales en tu zona.",
                "eventos":"Si asistes, prevé transporte y horarios con antelación.",
                "deporte":"Prevé cortes y afluencias; usa transporte público.",
                "salud":"Revisa pautas sanitarias locales y evita aglomeraciones.",
                "tecnología":"Actualiza sistemas y activa verificación en dos pasos.",
                "negocios":"Monitorea costes/ingresos y ajusta planes.",
                "justicia":"Toma nota; posibles efectos de cumplimiento.",
                "medioambiente":"Evita zonas afectadas; sigue avisos.",
                "otros":"Sin efecto directo en tu día a día."
              }
              adult=cw(adult_map.get(tag,"Sin efecto directo en tu día a día."),22)

              if sensitive:
                  teen="Información importante. Sigue indicaciones y no difundas rumores."
              else:
                  teen_map={
                    "movilidad":"Organízate: puede haber lío en calles/metro. 🚇",
                    "eventos":"Llega con tiempo: colas y tráfico. 🎟️",
                    "deporte":"Habrá mucha gente; metro mejor. ⚽",
                    "energía":"Ojo a precios: controla gasto. 💡",
                    "finanzas":"Atento a tu cuota: puede cambiar. 💸",
                    "clima":"Prepárate: clima duro, cuídate.",
                    "tecnología":"Actualiza apps y pon 2FA. 🔐",
                    "salud":"Cuídate y sigue consejos.",
                  }
                  teen=teen_map.get(tag,"Tranqui, solo para estar al día.")
              teen=cw(teen,18)
              if len(re.findall(r"[\U0001F300-\U0001FAFF\u2600-\u27BF]",teen))>1:
                  teen=re.sub(r"([\U0001F300-\U0001FAFF\u2600-\u27BF]).*$", r"\\1", teen)

              conf=0.7 if tag!="otros" else 0.5
              if conf<0.5:
                  adult,teen,sev,act="Sin efecto directo en tu día a día.","Sin efecto directo.",0,"FYI"

              def dedupe(src,ttl):
                  ttl5=" ".join(ttl.split()[:6]).lower()
                  return src if ttl5 not in src.lower() else "Resumen breve relevante para ti."
              adult=dedupe(adult,it["title"]); teen=dedupe(teen,it["title"])

              if tag not in TAGS: tag="otros"
              return {"adult_impact":adult,"teen_impact":teen,"tag":tag,"severity":sev,"horizon":hz,"action":act,"rationale":f"Reglas por keywords; sensible={'sí' if sensitive else 'no'}; fuente={it['source']}", "confidence":conf}

          # Geo scoring
          CITIES_CAT={"barcelona","bcn","girona","gerona","lleida","lerida","tarragona","sabadell","terrassa","badalona","hospitalet","l'hospitalet","mataró","mataro","reus","manresa","figueres","granollers","molins de rei","sitges","sants","eixample","gràcia","gracia"}
          CAT_TOKENS={"cataluña","catalunya","catalana","aliança catalana","orriols"}
          CITIES_RIOJA={"logroño","logrono","haro","calahorra","arnedo","najera","nàjera","albelda de iregua","albelda","la rioja","rioja"}
          FOREIGN={"estados unidos","eeuu","ee.uu.","francia","alemania","reino unido","uk","china","rusia","ucrania","italia","portugal","méxico","mexico","brasil","japón","japon"}
          WL_CAT={"elperiodico.com","lavanguardia.com","ccma.cat","ara.cat","naciodigital.cat","diaridebarcelona.cat","beteve.cat"}
          WL_RIOJA={"larioja.com","rioja2.com","nuevecuatrouno.com","larioja.org"}
          WL_ES={"elpais.com","rtve.es","elmundo.es","abc.es","eldiario.es","elespanol.com"}

          def score_region(text,host,region):
              t=text.lower(); s=0
              if region=="cataluna":
                  if any(w in t for w in CITIES_CAT): s+=3
                  if any(w in t for w in CAT_TOKENS): s+=3
                  if host in WL_CAT: s+=2
              elif region=="rioja":
                  if any(w in t for w in CITIES_RIOJA): s+=3
                  if host in WL_RIOJA: s+=2
              elif region=="espana":
                  base=2 if ("españa" in t or "espana" in t) else 0
                  if any(w in t for w in CAT_TOKENS): base=min(base,1)
                  s+=base
                  if host in WL_ES: s+=2
              if s<2 and any(w in t for w in FOREIGN): s-=2
              return s

          def best_region(item):
              text=f"{item['title']} {item['summary']}"; host=item["source"]
              scores={"cataluna":score_region(text,host,"cataluna"),
                      "rioja":score_region(text,host,"rioja"),
                      "espana":score_region(text,host,"espana"),
                      "global":0}
              region,sc=max(scores.items(), key=lambda kv: kv[1])
              return region if sc>=2 else "global"

          union=[]; bycat={}
          for cat,urls in FEEDS.items():
              all_it=[]
              for u in urls:
                  try:
                      text=robust_text(u); host=urllib.parse.urlparse(u).hostname
                      all_it.extend(parse(text,host))
                  except Exception as e:
                      print("Feed error:", u, e)
              bycat[cat]=all_it; union.extend(all_it)

          buckets={"cataluna":[], "espana":[], "rioja":[], "global":[]}
          def norm(t):
              t=(t or "").lower(); t=re.sub(r"[^\w\sáéíóúñüç]"," ",t); t=re.sub(r"\s+"," ",t).strip(); return t[:120]
          seen=set()
          for it in union:
              key=(norm(it["title"]), it["url"])
              if key in seen: continue
              seen.add(key)
              buckets[best_region(it)].append(it)

          def intersect(lst):
              bag={}
              for it in lst:
                  k=norm(it["title"])
                  if k not in bag: bag[k]={"it":it,"cnt":0}
                  bag[k]["cnt"]+=1
              return [v["it"] for v in sorted(bag.values(), key=lambda x:x["cnt"], reverse=True)]

          ranked={cat:intersect(lst) for cat,lst in buckets.items()}
          def backfill(cat):
              need=MAX-len(ranked[cat])
              if need<=0:return
              more=buckets[cat]
              for it in more:
                  if it in ranked[cat]: continue
                  ranked[cat].append(it)
                  if len(ranked[cat])>=MAX: break
          for cat in ["cataluna","rioja","espana","global"]:
              backfill(cat)

          def map_item(cat,it):
              meta=impact(it)
              t=(it["title"]+" "+it["summary"]).lower()
              if re.search(r"narcotr[aá]fico|detenido|operaci[oó]n policial", t): meta["tag"]="seguridad"
              if re.search(r"receta|gastronom[ií]a|plato|postre|origen .*catalana", t): meta["tag"]="cultura"
              return {
                "title": it["title"], "summary": it["summary"], "url": it["url"],
                "source": it["source"], "published_at": it["published_at"], "img":"",
                "location": "global" if cat=="global" else ("ES" if cat=="espana" else ("Cataluña" if cat=="cataluna" else "La Rioja")),
                "category": meta["tag"], **meta
              }

          out={
            "updated_at": datetime.now(tz=timezone.utc).isoformat(),
            "version":"live",
            "cataluna":[map_item("cataluna",it) for it in ranked["cataluna"][:MAX]],
            "espana":[map_item("espana",it) for it in ranked["espana"][:MAX]],
            "rioja":[map_item("rioja",it) for it in ranked["rioja"][:MAX]],
            "global":[map_item("global",it) for it in ranked["global"][:MAX]],
          }

          for cat in out:
              if isinstance(out[cat], list):
                  for it in out[cat]:
                      for k in ['title','summary','adult_impact','teen_impact','source']:
                          if k in it: it[k] = clean_txt(it[k])

          with open('data/latest.json','w',encoding='utf-8') as f:
              json.dump(out,f,ensure_ascii=False,indent=2)
          print("Wrote data/latest.json")
          PY

      - name: Copy to site/data for static serving
        run: |
          cp -f data/latest.json site/data/latest.json
          echo "Copied data/latest.json -> site/data/latest.json"

      - name: Upload latest.json artifact
        uses: actions/upload-artifact@v4
        with:
          name: latest-json
          path: data/latest.json

      - name: Commit both JSONs
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add data/latest.json site/data/latest.json
          git commit -m "chore: update latest.json $(date -u +%FT%TZ)" || echo "No changes"
          git push || true
